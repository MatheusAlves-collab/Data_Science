{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Entropia em Árvores de Decisão:\n",
    "\n",
    "A entropia é uma medida de desordem ou incertezas, é uma medida que varia de 0 (ordem, puro) até 1 (desordem).\n",
    "\n",
    "No contexto de árvores de decisão, a entropia é usada para avaliar a pureza de um nó. Cada nó representa uma divisão nos dados com base em um atributo (classe de alguma coluna). A fórmula para calcular a entropia de um nó é dada por:\n",
    "\n",
    "$$ E(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $$\n",
    "\n",
    "Onde $ E(S) $ é a entropia do nó, $ c $ é o número de classes, e $ p_i $ é a proporção de exemplos no nó pertencentes à classe $ i $. **Quanto mais próximo $ E(S) $ estiver de zero, mais puro é o nó, melhor ele discrimina a variável target**.\n",
    "\n",
    "### 1.1 Ganho de Informação:\n",
    "\n",
    "O objetivo em árvores de decisão é encontrar a melhor maneira de dividir os dados para maximizar a homogeneidade nos nós filhos. O ganho de informação é usado para medir a eficácia de uma divisão. É calculado subtraindo a entropia ponderada dos nós filhos da entropia do nó pai:\n",
    "\n",
    "$$ \\text{Ganho de Informação} = E(\\text{Nó Pai}) - \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot E(\\text{Nó Filho}_j) $$\n",
    "\n",
    "Onde $ k $ é o número de nós filhos, $ N_j $ é o número de amostras no $ j $-ésimo nó filho e $ N $ é o número total de amostras no nó pai. **Quanto maior o Ganho de Informação melhor**.\n",
    "\n",
    "### 1.2 Critério de Parada:\n",
    "\n",
    "Durante a construção da árvore, a divisão é interrompida quando um determinado critério é atendido, como atingir uma profundidade máxima, ter um número mínimo de exemplos em um nó ou alcançar um ganho de informação insuficiente.\n",
    "\n",
    "Em resumo, em árvores de decisão, a entropia é usada para avaliar a impureza de um conjunto de dados e guiar as decisões de divisão para criar uma árvore que melhor classifica os dados de maneira eficiente. O algoritmo procura dividir os dados de uma maneira que maximize o ganho de informação e, assim, minimize a entropia nos nós filhos e saber qual a ordem que cada várivael preditora irá aparecer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Quando a base é desbalanceada </h3>\n",
    "\n",
    "Se o conjunto de dados é balanceado, significa que as classes da variável target têm aproximadamente a mesma quantidade de casos. Nesse caso, a entropia inicial do conjunto de dados pode ser relativamente alta, já que não há uma classe dominante que tornaria o conjunto mais homogêneo. Além disso as divisões podem ter dificuldade em aumentar significativamente o ganho de informação, pois não há uma classe dominante que proporcione uma direção clara para a divisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Índice Gini para Árvore de Decisão\n",
    "\n",
    "O índice de Gini é outro critério utilizado em árvores de decisão para medir a impureza de um nó. Ele é frequentemente usado em alternância com a entropia, e ambos os critérios buscam encontrar divisões nos dados que resultem em subconjuntos mais homogêneos\n",
    "\n",
    "A fórmula para calcular o índice Gini $Gini(t)$ de um nó $t$ com $K$ classes é dada por:\n",
    "\n",
    "$$ Gini(t) = 1 - \\sum_{i=1}^{K} p_i^2 $$\n",
    "\n",
    "Onde $p_i$ é a proporção de exemplos da classe $i$ no nó $t$. Quanto menor o índice Gini, mais homogêneo é o nó.\n",
    "\n",
    "### 2.1 Ganho Gini para Árvore de Decisão\n",
    "\n",
    "O ganho Gini é usado para avaliar a eficácia de uma divisão em um nó. Ele é calculado subtraindo o índice Gini ponderado dos nós filhos do índice Gini do nó pai:\n",
    "\n",
    "$$  Gini(\\text{Nó Pai}) = \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot Gini(\\text{Nó Filho}_j) $$\n",
    "\n",
    "Onde $k$ é o número de nós filhos, $N_j$ é o número de exemplos no $j$-ésimo nó filho e $N$ é o número total de exemplos no nó pai. A escolha da divisão é feita maximizando o ganho Gini.\n",
    "\n",
    "**Quanto menor o Índice Gini do Nó melhor**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
