{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Entropia em Árvores de Decisão:\n",
    "\n",
    "A entropia é uma medida de desordem ou incertezas, ela alterna de 0 (ordem, puro) até 1 (desordem).\n",
    "\n",
    "No contexto de árvores de decisão, a entropia é usada para avaliar a pureza de um nó. Cada nó representa uma divisão nos dados com base em um atributo (classe de alguma coluna). A fórmula para calcular a entropia de um nó é dada por:\n",
    "\n",
    "$$ E(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $$\n",
    "\n",
    "Onde $ E(S) $ é a entropia do nó, $ c $ é o número de classes, e $ p_i $ é a proporção de exemplos no nó pertencentes à classe $ i $. **Quanto mais próximo $ E(S) $ estiver de zero, mais puro é o nó, melhor ele discrimina a variável target**.\n",
    "\n",
    "### 1.1 Ganho de Informação:\n",
    "\n",
    "O objetivo em árvores de decisão é encontrar a melhor maneira de dividir os dados para maximizar a homogeneidade nos nós filhos. O ganho de informação é usado para medir a eficácia de uma divisão. É calculado subtraindo a entropia ponderada dos nós filhos da entropia do nó pai:\n",
    "\n",
    "$$ \\text{Ganho de Informação} = E(\\text{Nó Pai}) - \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot E(\\text{Nó Filho}_j) $$\n",
    "\n",
    "Onde $ k $ é o número de nós filhos, $ N_j $ é o número de amostras no $ j $-ésimo nó filho e $ N $ é o número total de amostras no nó pai. **Quanto maior o Ganho de Informação melhor**.\n",
    "\n",
    "### 1.2 Critério de Parada:\n",
    "\n",
    "Durante a construção da árvore, a divisão é interrompida quando um determinado critério é atendido, como atingir uma profundidade máxima (quantos nós teremos), ter um número mínimo de amostras em um nó ou alcançar um ganho de informação insuficiente.\n",
    "\n",
    "Em resumo, em árvores de decisão, a entropia é usada para avaliar a impureza de um conjunto de dados e guiar as decisões de divisão para criar uma árvore que melhor classifica os dados de maneira eficiente. O algoritmo procura dividir os dados de uma maneira que maximize o ganho de informação e, assim, minimize a entropia nos nós filhos e saber qual a ordem que cada várivael preditora irá aparecer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Quando a base é desbalanceada </h3>\n",
    "\n",
    "Se o conjunto de dados é balanceado, significa que as classes da variável target têm aproximadamente a mesma quantidade de casos. Nesse caso, a entropia inicial do conjunto de dados pode ser relativamente alta, já que não há uma classe dominante que tornaria o conjunto mais homogêneo. Além disso as divisões podem ter dificuldade em aumentar significativamente o ganho de informação, pois não há uma classe dominante que proporcione uma direção clara para a divisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Índice Gini para Árvore de Decisão\n",
    "\n",
    "O índice de Gini é outro critério utilizado em árvores de decisão para medir a impureza de um nó. Ele é frequentemente usado em alternância com a entropia, e ambos os critérios buscam encontrar divisões nos dados que resultem em subconjuntos mais homogêneos.\n",
    "\n",
    "A fórmula para calcular o índice Gini $Gini(t)$ de um nó $t$ com $K$ classes é dada por:\n",
    "\n",
    "$$ Gini(t) = 1 - \\sum_{i=1}^{K} p_i^2 $$\n",
    "\n",
    "Onde $p_i$ é a proporção de exemplos da classe $i$ no nó $t$. Quanto menor o índice Gini, mais homogêneo é o nó.\n",
    "\n",
    "### 2.1 Ganho Gini para Árvore de Decisão\n",
    "\n",
    "O ganho Gini é usado para avaliar a eficácia de uma divisão em um nó. Ele é calculado subtraindo o índice Gini ponderado dos nós filhos do índice Gini do nó pai:\n",
    "\n",
    "$$  Gini(\\text{Nó Pai}) = \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot Gini(\\text{Nó Filho}_j) $$\n",
    "\n",
    "Onde $k$ é o número de nós filhos, $N_j$ é o número de exemplos no $j$-ésimo nó filho e $N$ é o número total de exemplos no nó pai. A escolha da divisão é feita maximizando o ganho Gini.\n",
    "\n",
    "**Quanto menor o Índice Gini do Nó melhor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudo de Parâmetros\n",
    "\n",
    "**max_depth:** Define a profundidade máxima da árvore. Isso limita o número de níveis ou ramos.\n",
    "\n",
    "**min_samples_split:** Especifica o número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "**min_samples_leaf:** Define o número mínimo de amostras necessário em uma folha (nó final).\n",
    "\n",
    " \"split\" ocorre quando um nó é dividido em subnós, enquanto \"leaf\" é um nó final que não é dividido mais, representando uma decisão final ou um valor previsto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sepal length (cm) sepal width (cm) petal length (cm) petal width (cm)\n",
       "0               5.1              3.5               1.4              0.2\n",
       "1               4.9              3.0               1.4              0.2\n",
       "2               4.7              3.2               1.3              0.2\n",
       "3               4.6              3.1               1.5              0.2\n",
       "4               5.0              3.6               1.4              0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "x_iris = pd.DataFrame(iris.data, columns= [iris.feature_names])\n",
    "y_iris = pd.Series(iris.target)\n",
    "\n",
    "x_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Separando dados em folds\n",
    "skfold = StratifiedKFold(n_splits=5, random_state=8, shuffle=True)\n",
    "\n",
    "#Criação do modelo\n",
    "modelo = DecisionTreeClassifier()\n",
    "resultado = cross_val_score(modelo, x_iris, y_iris, cv = skfold)\n",
    "\n",
    "#Acurácia\n",
    "print(\"Acurácia:\",round(resultado.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "arquivo = 'example.dot'\n",
    "modelo.fit(x_iris, y_iris)\n",
    "\n",
    "export_graphviz(modelo, out_file=arquivo, feature_names= iris.feature_names)\n",
    "with open(arquivo) as aberto:\n",
    "    grafico_dot = aberto.read()\n",
    "h = graphviz.Source(grafico_dot)\n",
    "h.view()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
