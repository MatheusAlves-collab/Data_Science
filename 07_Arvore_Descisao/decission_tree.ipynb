{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Árvore de Decisão\n",
    "\n",
    "Um algoritmo de árvore de decisão é um método de aprendizado de máquina supervisionado usado para classificação e regressão. Aqui está um resumo básico do algoritmo:\n",
    "\n",
    "**Construção da Árvore**:\n",
    "   - Começa com um nó raiz que contém todo o conjunto de dados.\n",
    "   - Escolhe o melhor atributo para dividir os dados com base em critérios como entropia, ganho de informação ou índice Gini.\n",
    "   - Divide o conjunto de dados em subconjuntos com base nos valores do atributo selecionado.\n",
    "   - Repete recursivamente esse processo para cada nó filho, até que:\n",
    "     - Todos os pontos de dados em um nó pertençam à mesma classe (no caso de classificação) ou alcançar um critério de parada (por exemplo, profundidade máxima da árvore).\n",
    "   - É possível também podar a arvoré para generalizar mais seus resultados, evitando overfitting, podendo ser pré-poda (parando a construção da árvore mais cedo) ou pós-poda (removendo partes da árvore após a construção)\n",
    "\n",
    "Em resumo, as árvores de decisão são poderosas ferramentas de aprendizado de máquina, conhecidas por sua capacidade de lidar com interpretação e facilidade de uso. No entanto, elas requerem cuidados adequados para evitar problemas de overfitting e podem não ser a melhor escolha em todos os casos de modelagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Entropia:\n",
    "\n",
    "A entropia é uma medida de desordem ou incertezas, ela alterna de 0 (ordem, puro) até 1 (desordem).\n",
    "\n",
    "No contexto de árvores de decisão, a entropia é usada para avaliar a pureza de um nó. Cada nó representa uma divisão nos dados com base em um atributo (classe de alguma coluna). A fórmula para calcular a entropia de um nó é dada por:\n",
    "\n",
    "$$ E(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $$\n",
    "\n",
    "Onde $ E(S) $ é a entropia do nó, $ c $ é o número de classes, e $ p_i $ é a proporção de exemplos no nó pertencentes à classe $ i $. **Quanto mais próximo $ E(S) $ estiver de zero, mais puro é o nó, melhor ele discrimina a variável target**.\n",
    "\n",
    "### 1.2 Ganho de Informação:\n",
    "\n",
    "O objetivo em árvores de decisão é encontrar a melhor maneira de dividir os dados para maximizar a homogeneidade nos nós filhos. O ganho de informação é usado para medir a eficácia de uma divisão. É calculado subtraindo a entropia ponderada dos nós filhos da entropia do nó pai:\n",
    "\n",
    "$$ \\text{Ganho de Informação} = E(\\text{Nó Pai}) - \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot E(\\text{Nó Filho}_j) $$\n",
    "\n",
    "Onde $ k $ é o número de nós filhos, $ N_j $ é o número de amostras no $ j $-ésimo nó filho e $ N $ é o número total de amostras no nó pai. **Quanto maior o Ganho de Informação melhor**.\n",
    "\n",
    "### 1.3 Índice Gini\n",
    "\n",
    "O índice de Gini é outro critério utilizado em árvores de decisão para medir a impureza de um nó. Ele é frequentemente usado em alternância com a entropia, e ambos os critérios buscam encontrar divisões nos dados que resultem em subconjuntos mais homogêneos.\n",
    "\n",
    "A fórmula para calcular o índice Gini $Gini(t)$ de um nó $t$ com $K$ classes é dada por:\n",
    "\n",
    "$$ Gini(t) = 1 - \\sum_{i=1}^{K} p_i^2 $$\n",
    "\n",
    "Onde $p_i$ é a proporção de exemplos da classe $i$ no nó $t$. Quanto menor o índice Gini, mais homogêneo é o nó.\n",
    "\n",
    "### 1.4 Ganho Gini\n",
    "\n",
    "O ganho Gini é usado para avaliar a eficácia de uma divisão em um nó. Ele é calculado subtraindo o índice Gini ponderado dos nós filhos do índice Gini do nó pai:\n",
    "\n",
    "$$  Gini(\\text{Nó Pai}) = \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot Gini(\\text{Nó Filho}_j) $$\n",
    "\n",
    "Onde $k$ é o número de nós filhos, $N_j$ é o número de exemplos no $j$-ésimo nó filho e $N$ é o número total de exemplos no nó pai. A escolha da divisão é feita maximizando o ganho Gini.\n",
    "\n",
    "**Quanto menor o Índice Gini do Nó melhor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critério de Parada:\n",
    "\n",
    "Durante a construção da árvore, a divisão é interrompida quando um determinado critério é atendido, como atingir uma profundidade máxima (quantos nós teremos), ter um número mínimo de amostras em um nó ou alcançar um ganho de informação insuficiente.\n",
    "\n",
    "Em resumo, em árvores de decisão, a entropia é usada para avaliar a impureza de um conjunto de dados e guiar as decisões de divisão para criar uma árvore que melhor classifica os dados de maneira eficiente. O algoritmo procura dividir os dados de uma maneira que maximize o ganho de informação e, assim, minimize a entropia nos nós filhos e saber qual a ordem que cada várivael preditora irá aparecer.\n",
    "\n",
    "<h3>Quando a base é desbalanceada </h3>\n",
    "\n",
    "Se o conjunto de dados é balanceado, significa que as classes da variável target têm aproximadamente a mesma quantidade de casos. Nesse caso, a entropia inicial do conjunto de dados pode ser relativamente alta, já que não há uma classe dominante que tornaria o conjunto mais homogêneo. Além disso as divisões podem ter dificuldade em aumentar significativamente o ganho de informação, pois não há uma classe dominante que proporcione uma direção clara para a divisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sepal length (cm) sepal width (cm) petal length (cm) petal width (cm)\n",
       "0               5.1              3.5               1.4              0.2\n",
       "1               4.9              3.0               1.4              0.2\n",
       "2               4.7              3.2               1.3              0.2\n",
       "3               4.6              3.1               1.5              0.2\n",
       "4               5.0              3.6               1.4              0.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "x_iris = pd.DataFrame(iris.data, columns= [iris.feature_names])\n",
    "y_iris = pd.Series(iris.target)\n",
    "\n",
    "x_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Separando dados em folds\n",
    "skfold = StratifiedKFold(n_splits=5, random_state=8, shuffle=True)\n",
    "\n",
    "#Criação do modelo\n",
    "modelo = DecisionTreeClassifier()\n",
    "resultado = cross_val_score(modelo, x_iris, y_iris, cv = skfold)\n",
    "\n",
    "#Acurácia\n",
    "print(\"Acurácia:\",round(resultado.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "arquivo = 'example.dot'\n",
    "modelo.fit(x_iris, y_iris)\n",
    "\n",
    "export_graphviz(modelo, out_file=arquivo, feature_names= iris.feature_names)\n",
    "with open(arquivo) as aberto:\n",
    "    grafico_dot = aberto.read()\n",
    "h = graphviz.Source(grafico_dot)\n",
    "h.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudo de Parâmetros\n",
    "\n",
    "**max_depth:** Define a profundidade máxima da árvore. Isso limita o número de níveis ou ramos.\n",
    "\n",
    "**min_samples_split:** Especifica o número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "**min_samples_leaf:** Define o número mínimo de amostras necessário em uma folha (nó final).\n",
    "\n",
    " \"split\" ocorre quando um nó é dividido em subnós, enquanto \"leaf\" é um nó final que não é dividido mais, representando uma decisão final ou um valor previsto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo Split: 2\n",
      "Máxima Profundidade: 4\n",
      "Algoritmo Escolhido: gini\n",
      "Acurácia: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parametros_grid = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [3,4,5,6],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8]\n",
    "    ##'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "modelo_1 = DecisionTreeClassifier(random_state= 10)\n",
    "\n",
    "gridDecisionTree = GridSearchCV(estimator= modelo_1, param_grid= parametros_grid, cv = 5, scoring='accuracy')\n",
    "gridDecisionTree.fit(x_iris, y_iris)\n",
    "\n",
    "#Imprimindo melhores parâmetros\n",
    "print(\"Mínimo Split:\", gridDecisionTree.best_estimator_.min_samples_split)\n",
    "print(\"Máxima Profundidade:\",gridDecisionTree.best_estimator_.max_features_)\n",
    "print(\"Algoritmo Escolhido:\",gridDecisionTree.best_estimator_.criterion)\n",
    "print(\"Acurácia:\",round(gridDecisionTree.best_score_,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhor_modelo = DecisionTreeClassifier(min_samples_split= 2, max_depth=4, criterion='gini', scoring='accuracy')\n",
    "melhor_modelo.fit(x_iris,y_iris)\n",
    "\n",
    "export_graphviz(melhor_modelo, out_file='exemplo2.dot', feature_names= iris.feature_names)\n",
    "with open('exemplo2.dot') as aberto:\n",
    "    grafico_dot = aberto.read()\n",
    "h = graphviz.Source(grafico_dot)\n",
    "h.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente de determinaçãoR2: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Dados Admissão de Alunos\n",
    "df_adm = pd.read_csv('C:/GIT/Data_Science/01_Regressao_linear/Admission_Predict.csv')\n",
    "\n",
    "df_admiss = (df_adm.drop(['Serial No.'], axis='columns')).copy()\n",
    "\n",
    "x_admiss = df_admiss.loc[:, df_admiss.columns != 'Chance of Admit ']\n",
    "y_admiss = df_admiss.loc[:, df_admiss.columns == 'Chance of Admit ']\n",
    "\n",
    "# Separando em folds\n",
    "kfold = KFold(n_splits=5, random_state=7, shuffle=True)\n",
    "\n",
    "modelo_2 = DecisionTreeRegressor()\n",
    "resultado = cross_val_score(modelo_2, x_admiss, y_admiss, cv = kfold)\n",
    "\n",
    "print(\"Coeficiente de determinaçãoR2:\", round(resultado.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo Split: 5\n",
      "Máxima Profundidade: 7\n",
      "Algoritmo Escolhido: friedman_mse\n",
      "Coeficiente R2: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Definindo Parâmetros\n",
    "parametros_grid_1 = {\n",
    "    'criterion': ['mse','friedman_mse','mae'],\n",
    "    'max_depth': [3,4,5,6,7],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8]\n",
    "    ##'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Modelo\n",
    "modelo_3 = DecisionTreeRegressor()\n",
    "\n",
    "# Criando Grids\n",
    "GridSearchRegressor = GridSearchCV(estimator= modelo_3,  cv = 5, param_grid = parametros_grid_1)\n",
    "GridSearchRegressor.fit(x_admiss, y_admiss)\n",
    "\n",
    "#Imprimindo melhores parâmetros\n",
    "print(\"Mínimo Split:\", GridSearchRegressor.best_estimator_.min_samples_split)\n",
    "print(\"Máxima Profundidade:\",GridSearchRegressor.best_estimator_.max_features_)\n",
    "print(\"Algoritmo Escolhido:\",GridSearchRegressor.best_estimator_.criterion)\n",
    "print(\"Coeficiente R2:\",round(GridSearchRegressor.best_score_,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
